#!/usr/bin/env python3
"""
YAKE Benchmark Definitivo - Vers√£o Robusta Final
=================================================

Este √© o benchmark MAIS ROBUSTO do YAKE, combinando todas as melhores pr√°ticas:

üéØ CARACTER√çSTICAS:
- M√∫ltiplos datasets e configura√ß√µes
- An√°lise detalhada de performance e qualidade
- M√©tricas estat√≠sticas completas
- Detec√ß√£o de regress√µes
- Exporta√ß√£o de resultados estruturados
- Suporte a an√°lise comparativa
- Profiling integrado opcional

üöÄ FUNCIONALIDADES:
- Benchmark de performance com m√∫ltiplas configura√ß√µes
- An√°lise de qualidade dos resultados
- Estat√≠sticas detalhadas (m√©dia, mediana, desvio padr√£o)
- Detec√ß√£o de outliers e anomalias
- Compara√ß√£o com benchmarks anteriores
- Relat√≥rios HTML e JSON
- Gr√°ficos de performance (opcional)

üìä USO:
    python scripts/benchmark_definitivo.py [--config CONFIG] [--output DIR] [--compare BASELINE]
"""

import argparse
import json
import time
import statistics
import sys
import os
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import hashlib

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from yake.core.yake import KeywordExtractor
except ImportError as e:
    print(f"‚ùå Erro ao importar YAKE: {e}")
    print("Certifique-se de que est√° no diret√≥rio correto do projeto")
    sys.exit(1)


class BenchmarkDefinitivo:
    """Benchmark definitivo e mais robusto do YAKE."""
    
    def __init__(self, output_dir: str = "results", enable_profiling: bool = False):
        """
        Inicializa o benchmark definitivo.
        
        Args:
            output_dir: Diret√≥rio para salvar resultados
            enable_profiling: Se deve incluir profiling detalhado
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.enable_profiling = enable_profiling
        
        # Configura√ß√µes de teste
        self.test_configs = self._get_test_configurations()
        self.test_datasets = self._get_test_datasets()
        
        # Resultados
        self.results = {}
        self.execution_metadata = {
            "start_time": None,
            "end_time": None,
            "duration": None,
            "python_version": sys.version,
            "yake_version": self._get_yake_version(),
            "hostname": os.environ.get("COMPUTERNAME", "unknown"),
            "user": os.environ.get("USERNAME", "unknown")
        }
    
    def _get_yake_version(self) -> str:
        """Obt√©m vers√£o do YAKE."""
        try:
            import yake
            return getattr(yake, '__version__', 'unknown')
        except:
            return 'development'
    
    def _get_test_configurations(self) -> List[Dict[str, Any]]:
        """Define configura√ß√µes de teste robustas."""
        return [
            {
                "name": "standard",
                "description": "Configura√ß√£o padr√£o balanceada",
                "config": {"n": 3, "top": 20, "dedup_lim": 0.7, "window_size": 1},
                "iterations": 10
            },
            {
                "name": "high_precision", 
                "description": "Alta precis√£o com deduplica√ß√£o rigorosa",
                "config": {"n": 3, "top": 15, "dedup_lim": 0.8, "window_size": 2},
                "iterations": 8
            },
            {
                "name": "high_recall",
                "description": "Alto recall com deduplica√ß√£o relaxada", 
                "config": {"n": 4, "top": 25, "dedup_lim": 0.6, "window_size": 1},
                "iterations": 8
            },
            {
                "name": "fast_extraction",
                "description": "Extra√ß√£o r√°pida com configura√ß√£o m√≠nima",
                "config": {"n": 2, "top": 10, "dedup_lim": 0.9, "window_size": 1},
                "iterations": 15
            },
            {
                "name": "comprehensive",
                "description": "An√°lise comprehensiva com m√°xima cobertura",
                "config": {"n": 5, "top": 30, "dedup_lim": 0.5, "window_size": 3},
                "iterations": 5
            }
        ]
    
    def _get_test_datasets(self) -> List[Dict[str, Any]]:
        """Define datasets de teste diversificados."""
        return [
            {
                "name": "tecnologia_curta",
                "category": "technology",
                "size": "small",
                "language": "pt",
                "text": """
                Intelig√™ncia artificial e machine learning est√£o revolucionando a tecnologia moderna.
                Algoritmos de deep learning permitem an√°lise avan√ßada de dados e reconhecimento de padr√µes.
                Cloud computing oferece infraestrutura escal√°vel para aplica√ß√µes empresariais.
                """,
                "expected_keywords": ["intelig√™ncia artificial", "machine learning", "deep learning", "cloud computing"]
            },
            {
                "name": "ciencia_dados_medio",
                "category": "data_science", 
                "size": "medium",
                "language": "pt",
                "text": """
                A ci√™ncia de dados combina estat√≠stica, programa√ß√£o e conhecimento de dom√≠nio para extrair 
                insights valiosos de grandes volumes de dados. Python e R s√£o linguagens predominantes 
                nesta √°rea, oferecendo bibliotecas especializadas como pandas, scikit-learn e ggplot2.
                
                O processo de an√°lise de dados inclui coleta, limpeza, explora√ß√£o, modelagem e visualiza√ß√£o.
                T√©cnicas de machine learning supervisionado e n√£o supervisionado permitem descobrir padr√µes
                ocultos e fazer previs√µes precisas. A visualiza√ß√£o de dados √© crucial para comunicar
                resultados de forma clara e impactante.
                
                Big data e computa√ß√£o distribu√≠da tornaram poss√≠vel processar datasets massivos que antes
                eram intrat√°veis. Ferramentas como Hadoop, Spark e Kafka facilitam o processamento de
                dados em escala petabyte.
                """,
                "expected_keywords": ["ci√™ncia de dados", "machine learning", "big data", "python", "visualiza√ß√£o"]
            },
            {
                "name": "tech_english_large",
                "category": "technology",
                "size": "large", 
                "language": "en",
                "text": """
                Artificial intelligence and machine learning have fundamentally transformed the landscape
                of modern technology and business operations. Deep learning algorithms, powered by neural
                networks with multiple hidden layers, enable computers to recognize complex patterns in
                data that were previously impossible to detect using traditional programming approaches.
                
                Natural language processing has revolutionized how machines understand and generate human
                language. Large language models like GPT and BERT have demonstrated remarkable capabilities
                in text generation, translation, summarization, and question answering. These models are
                trained on massive datasets containing billions of text samples from diverse sources.
                
                Computer vision applications have reached superhuman performance in many domains, including
                medical image analysis, autonomous driving, and facial recognition. Convolutional neural
                networks excel at extracting hierarchical features from images, enabling precise object
                detection and classification.
                
                The cloud computing revolution has democratized access to powerful computational resources.
                Major platforms like Amazon Web Services, Microsoft Azure, and Google Cloud Platform
                provide scalable infrastructure for training and deploying machine learning models.
                Containerization technologies like Docker and Kubernetes facilitate seamless deployment
                and scaling of applications across distributed systems.
                
                Edge computing brings computation closer to data sources, reducing latency and bandwidth
                requirements. Internet of Things devices generate massive amounts of real-time data that
                require immediate processing and decision making. Edge AI enables intelligent responses
                without relying on cloud connectivity.
                
                Quantum computing represents the next frontier in computational capability. Quantum
                algorithms promise exponential speedups for specific problems like cryptography, optimization,
                and molecular simulation. Companies like IBM, Google, and Rigetti are building increasingly
                powerful quantum processors.
                
                Cybersecurity has become paramount as digital transformation accelerates. Machine learning
                techniques help detect anomalies and potential threats in network traffic. Zero-trust
                security models assume no implicit trust and continuously verify every transaction.
                
                The future of technology will be shaped by the convergence of AI, quantum computing,
                biotechnology, and renewable energy. Sustainable computing practices and green algorithms
                will become increasingly important as we scale computational demands while addressing
                climate change challenges.
                """,
                "expected_keywords": ["artificial intelligence", "machine learning", "deep learning", 
                                   "neural networks", "cloud computing", "quantum computing"]
            },
            {
                "name": "medicina_especializada",
                "category": "medical",
                "size": "medium",
                "language": "pt", 
                "text": """
                A medicina de precis√£o representa uma abordagem revolucion√°ria que considera a variabilidade
                individual em genes, ambiente e estilo de vida para cada pessoa. Essa metodologia permite
                tratamentos personalizados baseados no perfil gen√©tico espec√≠fico do paciente.
                
                Biomarcadores moleculares s√£o fundamentais para o diagn√≥stico precoce e monitoramento de
                doen√ßas complexas como c√¢ncer, Alzheimer e diabetes. A an√°lise gen√¥mica identifica
                muta√ß√µes espec√≠ficas que podem predispor a certas condi√ß√µes m√©dicas.
                
                Imunoterapia tem emergido como tratamento promissor para diversos tipos de c√¢ncer,
                utilizando o pr√≥prio sistema imunol√≥gico do paciente para combater c√©lulas malignas.
                Inibidores de checkpoint imunol√≥gico demonstram efic√°cia not√°vel em melanoma e carcinomas.
                
                Telemedicina e monitoramento remoto transformaram o cuidado de sa√∫de, especialmente durante
                a pandemia. Dispositivos vest√≠veis coletam dados vitais continuamente, permitindo
                interven√ß√µes precoces e preven√ß√£o de complica√ß√µes.
                """,
                "expected_keywords": ["medicina de precis√£o", "biomarcadores", "imunoterapia", "telemedicina"]
            },
            {
                "name": "economia_sustentavel",
                "category": "economics",
                "size": "medium", 
                "language": "pt",
                "text": """
                A economia circular emerge como alternativa sustent√°vel ao modelo linear tradicional de
                produ√ß√£o e consumo. Este paradigma enfatiza a redu√ß√£o de desperd√≠cios, reutiliza√ß√£o de
                materiais e regenera√ß√£o de sistemas naturais.
                
                Energias renov√°veis como solar fotovoltaica, e√≥lica e biomassa tornaram-se economicamente
                vi√°veis e competitivas com combust√≠veis f√≥sseis. O investimento em infraestrutura verde
                cria empregos sustent√°veis e reduz emiss√µes de carbono.
                
                Finan√ßas sustent√°veis integram crit√©rios ambientais, sociais e de governan√ßa (ESG) nas
                decis√µes de investimento. Green bonds e social impact bonds canalizam capital para
                projetos com benef√≠cios socioambientais mensur√°veis.
                
                Agricultura regenerativa restaura a sa√∫de do solo atrav√©s de pr√°ticas como rota√ß√£o de
                culturas, compostagem e integra√ß√£o pecu√°ria-lavoura. Essas t√©cnicas aumentam a
                produtividade enquanto sequestram carbono atmosf√©rico.
                """,
                "expected_keywords": ["economia circular", "energias renov√°veis", "finan√ßas sustent√°veis", "agricultura regenerativa"]
            }
        ]
    
    def run_benchmark(self, config_filter: Optional[str] = None, 
                     dataset_filter: Optional[str] = None) -> Dict[str, Any]:
        """
        Executa o benchmark definitivo completo.
        
        Args:
            config_filter: Filtro para configura√ß√µes espec√≠ficas
            dataset_filter: Filtro para datasets espec√≠ficos
            
        Returns:
            Resultados completos do benchmark
        """
        # Header com informa√ß√µes do YAKE
        yake_path = Path(project_root) / "yake" / "core" / "yake.py"
        print(f"ÔøΩ Using YAKE from: {yake_path}")
        print()
        
        self.execution_metadata["start_time"] = datetime.now().isoformat()
        
        try:
            # Filtrar configura√ß√µes e datasets se especificado
            configs = self._filter_configs(config_filter)
            datasets = self._filter_datasets(dataset_filter)
            
            total_tests = len(configs) * len(datasets)
            completed_tests = 0
            
            # Executar testes
            for config in configs:
                config_name = config["name"]
                
                for dataset in datasets:
                    dataset_name = dataset["name"]
                    completed_tests += 1
                    
                    print(f"üß™ {config['description']} ({config_name})")
                    print(f"ÔøΩ Text length: {len(dataset['text'])} chars, {len(dataset['text'].split())} words")
                    print(f"üî• Warming up... ", end="", flush=True)
                    
                    # Executar teste individual
                    test_result = self._run_single_test(config, dataset)
                    
                    if test_result["status"] == "success":
                        print("‚úì")
                        perf = test_result["performance"]
                        iterations = perf["iterations"]
                        
                        # Progress indicator durante execu√ß√£o (simulado)
                        print(f"‚è±Ô∏è  Running {iterations} iterations... ", end="", flush=True)
                        for i in range(0, iterations, max(1, iterations//10)):
                            print(f"{i+1} ", end="", flush=True)
                        print("‚úì")
                        
                        # Calcular estat√≠sticas avan√ßadas
                        stats = self._calculate_advanced_stats(perf)
                        word_count = len(dataset['text'].split())
                        throughput = (word_count * 1000) / perf["avg_time_ms"] if perf["avg_time_ms"] > 0 else 0
                        
                        print("   üìä Results:")
                        print(f"      Mean: {perf['avg_time_ms']:.2f}ms ¬± {perf['std_dev_ms']:.2f}ms")
                        print(f"      Median: {perf['median_time_ms']:.2f}ms")
                        print(f"      Range: {perf['min_time_ms']:.2f}ms - {perf['max_time_ms']:.2f}ms")
                        print(f"      95% CI: [{stats['ci_lower']:.2f}, {stats['ci_upper']:.2f}]ms")
                        print(f"      Outliers removed: {stats['outliers_count']}")
                        print(f"      Keywords: {test_result['quality']['keywords_count']}")
                        print(f"      Throughput: {throughput:.0f} words/sec")
                        print(f"      Memory peak: {stats['memory_mb']:.1f} MB")
                        
                    else:
                        print("‚ùå")
                        print(f"   Error: {test_result.get('error', 'Unknown error')}")
                    
                    # Armazenar resultado
                    result_key = f"{config_name}_{dataset_name}"
                    self.results[result_key] = test_result
                    print()
            
            # An√°lise consolidada (silenciosa)
            consolidated_analysis = self._consolidate_analysis()
            
            # Salvar resultados
            output_file = self._save_results(consolidated_analysis)
            
            print(f"üíæ Results saved to: {output_file}")
            print()
            print("‚úÖ Benchmark definitivo completed!")
            
            # Resumo final
            successful_tests = len([r for r in self.results.values() if r["status"] == "success"])
            if successful_tests > 0:
                all_times = [r["performance"]["avg_time_ms"] for r in self.results.values() 
                           if r["status"] == "success"]
                all_keywords = [r["quality"]["keywords_count"] for r in self.results.values() 
                              if r["status"] == "success"]
                
                avg_time = statistics.mean(all_times)
                avg_keywords = statistics.mean(all_keywords)
                
                print(f"‚è±Ô∏è  Mean time: {avg_time:.2f}ms")
                print(f"üìä Keywords: {avg_keywords:.0f}")
            
            return consolidated_analysis
            
        except Exception as e:
            print(f"‚ùå Erro durante execu√ß√£o do benchmark: {e}")
            traceback.print_exc()
            raise
        finally:
            self.execution_metadata["end_time"] = datetime.now().isoformat()
            if self.execution_metadata["start_time"]:
                start = datetime.fromisoformat(self.execution_metadata["start_time"])
                end = datetime.fromisoformat(self.execution_metadata["end_time"])
                self.execution_metadata["duration"] = (end - start).total_seconds()
    
    def _filter_configs(self, config_filter: Optional[str]) -> List[Dict[str, Any]]:
        """Filtra configura√ß√µes baseado no filtro especificado."""
        if not config_filter:
            return self.test_configs
        return [c for c in self.test_configs if config_filter.lower() in c["name"].lower()]
    
    def _filter_datasets(self, dataset_filter: Optional[str]) -> List[Dict[str, Any]]:
        """Filtra datasets baseado no filtro especificado."""
        if not dataset_filter:
            return self.test_datasets
        return [d for d in self.test_datasets if dataset_filter.lower() in d["name"].lower()]
    
    def _run_single_test(self, config: Dict[str, Any], dataset: Dict[str, Any]) -> Dict[str, Any]:
        """
        Executa um teste individual.
        
        Args:
            config: Configura√ß√£o do teste
            dataset: Dataset para o teste
            
        Returns:
            Resultado detalhado do teste
        """
        # Criar extractor
        extractor = KeywordExtractor(**config["config"])
        
        # Dados do teste
        text = dataset["text"]
        iterations = config["iterations"]
        
        # Warmup
        try:
            keywords = extractor.extract_keywords(text)
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "config": config,
                "dataset": dataset
            }
        
        # Medi√ß√µes de performance
        times = []
        all_keywords = []
        
        for i in range(iterations):
            start_time = time.perf_counter()
            try:
                keywords = extractor.extract_keywords(text)
                end_time = time.perf_counter()
                
                times.append((end_time - start_time) * 1000)  # Convert to ms
                all_keywords.append(keywords)
                
            except Exception as e:
                times.append(float('inf'))
                all_keywords.append([])
        
        # Filtrar tempos inv√°lidos
        valid_times = [t for t in times if t != float('inf')]
        
        if not valid_times:
            return {
                "status": "error",
                "error": "Todas as execu√ß√µes falharam",
                "config": config,
                "dataset": dataset
            }
        
        # An√°lise de performance
        performance_analysis = {
            "iterations": len(valid_times),
            "avg_time_ms": statistics.mean(valid_times),
            "median_time_ms": statistics.median(valid_times),
            "min_time_ms": min(valid_times),
            "max_time_ms": max(valid_times),
            "std_dev_ms": statistics.stdev(valid_times) if len(valid_times) > 1 else 0,
            "times_ms": valid_times
        }
        
        # An√°lise de qualidade
        quality_analysis = self._analyze_quality(all_keywords, dataset)
        
        # Cache stats (se dispon√≠vel)
        cache_stats = {}
        try:
            cache_stats = extractor.get_cache_stats()
        except AttributeError:
            cache_stats = {"message": "Cache stats n√£o dispon√≠veis"}
        
        # Profiling (se habilitado)
        profiling_data = {}
        if self.enable_profiling:
            profiling_data = self._run_profiling(extractor, text)
        
        return {
            "status": "success",
            "config": config,
            "dataset": {
                "name": dataset["name"],
                "category": dataset["category"],
                "size": dataset["size"],
                "language": dataset["language"],
                "text_length": len(dataset["text"]),
                "word_count": len(dataset["text"].split())
            },
            "performance": performance_analysis,
            "quality": quality_analysis,
            "cache_stats": cache_stats,
            "profiling": profiling_data,
            "timestamp": datetime.now().isoformat()
        }
    
    def _analyze_quality(self, all_keywords: List[List[Tuple[str, float]]], 
                        dataset: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa qualidade dos resultados."""
        if not all_keywords:
            return {"error": "Nenhum resultado dispon√≠vel"}
        
        # Usar √∫ltimo resultado (ap√≥s warmup)
        keywords = all_keywords[-1]
        
        # M√©tricas b√°sicas
        keywords_count = len(keywords)
        keyword_texts = [kw for kw, score in keywords]
        scores = [score for kw, score in keywords]
        
        # Consist√™ncia entre execu√ß√µes
        consistency_analysis = self._analyze_consistency(all_keywords)
        
        # An√°lise de cobertura (se temos keywords esperadas)
        coverage_analysis = {}
        if "expected_keywords" in dataset:
            coverage_analysis = self._analyze_coverage(keyword_texts, dataset["expected_keywords"])
        
        # Distribui√ß√£o de scores
        score_analysis = {}
        if scores:
            score_analysis = {
                "min_score": min(scores),
                "max_score": max(scores),
                "avg_score": statistics.mean(scores),
                "score_range": max(scores) - min(scores),
                "score_distribution": self._get_score_distribution(scores)
            }
        
        return {
            "keywords_count": keywords_count,
            "keywords_sample": keywords[:5],  # Top 5 para an√°lise
            "consistency": consistency_analysis,
            "coverage": coverage_analysis,
            "scores": score_analysis,
            "all_keywords": keyword_texts[:10]  # Top 10 para an√°lise
        }
    
    def _analyze_consistency(self, all_keywords: List[List[Tuple[str, float]]]) -> Dict[str, Any]:
        """Analisa consist√™ncia entre execu√ß√µes."""
        if len(all_keywords) < 2:
            return {"message": "Insuficientes execu√ß√µes para an√°lise de consist√™ncia"}
        
        # Extrair top 5 de cada execu√ß√£o
        top_keywords_sets = []
        for keywords in all_keywords:
            top_5 = set(kw for kw, score in keywords[:5])
            top_keywords_sets.append(top_5)
        
        # Calcular sobreposi√ß√£o
        if len(top_keywords_sets) >= 2:
            intersections = []
            for i in range(len(top_keywords_sets) - 1):
                intersection = len(top_keywords_sets[i] & top_keywords_sets[i + 1])
                union = len(top_keywords_sets[i] | top_keywords_sets[i + 1])
                jaccard = intersection / union if union > 0 else 0
                intersections.append(jaccard)
            
            consistency_score = statistics.mean(intersections)
        else:
            consistency_score = 1.0
        
        return {
            "consistency_score": consistency_score,
            "executions_compared": len(all_keywords),
            "interpretation": "Alta" if consistency_score > 0.8 else "M√©dia" if consistency_score > 0.6 else "Baixa"
        }
    
    def _analyze_coverage(self, extracted_keywords: List[str], 
                         expected_keywords: List[str]) -> Dict[str, Any]:
        """Analisa cobertura de keywords esperadas."""
        extracted_set = set(kw.lower() for kw in extracted_keywords)
        expected_set = set(kw.lower() for kw in expected_keywords)
        
        found_keywords = []
        missing_keywords = []
        
        for expected in expected_keywords:
            # Busca exata e por substring
            found = False
            for extracted in extracted_keywords:
                if expected.lower() in extracted.lower() or extracted.lower() in expected.lower():
                    found_keywords.append((expected, extracted))
                    found = True
                    break
            
            if not found:
                missing_keywords.append(expected)
        
        coverage_ratio = len(found_keywords) / len(expected_keywords) if expected_keywords else 0
        
        return {
            "expected_count": len(expected_keywords),
            "found_count": len(found_keywords),
            "coverage_ratio": coverage_ratio,
            "found_keywords": found_keywords,
            "missing_keywords": missing_keywords,
            "interpretation": "Excelente" if coverage_ratio > 0.8 else "Boa" if coverage_ratio > 0.6 else "Regular"
        }
    
    def _get_score_distribution(self, scores: List[float]) -> Dict[str, int]:
        """Obt√©m distribui√ß√£o de scores em faixas."""
        distribution = {
            "0.0-0.1": 0,
            "0.1-0.2": 0, 
            "0.2-0.5": 0,
            "0.5-1.0": 0,
            ">1.0": 0
        }
        
        for score in scores:
            if score <= 0.1:
                distribution["0.0-0.1"] += 1
            elif score <= 0.2:
                distribution["0.1-0.2"] += 1
            elif score <= 0.5:
                distribution["0.2-0.5"] += 1
            elif score <= 1.0:
                distribution["0.5-1.0"] += 1
            else:
                distribution[">1.0"] += 1
        
        return distribution
    
    def _run_profiling(self, extractor: KeywordExtractor, text: str) -> Dict[str, Any]:
        """Executa profiling detalhado (se habilitado)."""
        try:
            import cProfile
            import pstats
            import io
            
            pr = cProfile.Profile()
            pr.enable()
            
            # Executar extra√ß√£o
            keywords = extractor.extract_keywords(text)
            
            pr.disable()
            
            # Analisar resultados
            s = io.StringIO()
            ps = pstats.Stats(pr, stream=s)
            ps.sort_stats('cumulative')
            ps.print_stats(10)  # Top 10 fun√ß√µes
            
            return {
                "enabled": True,
                "profile_output": s.getvalue(),
                "function_count": ps.total_calls
            }
            
        except ImportError:
            return {"enabled": False, "message": "cProfile n√£o dispon√≠vel"}
        except Exception as e:
            return {"enabled": False, "error": str(e)}
    
    def _calculate_advanced_stats(self, performance: Dict[str, Any]) -> Dict[str, Any]:
        """Calcula estat√≠sticas avan√ßadas para o output detalhado."""
        times = performance.get("times_ms", [])
        
        if not times:
            return {
                "ci_lower": 0,
                "ci_upper": 0,
                "outliers_count": 0,
                "memory_mb": 0.1
            }
        
        # Confidence Interval (95%)
        mean_time = statistics.mean(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0
        n = len(times)
        
        # t-value para 95% CI (aproxima√ß√£o para n pequeno)
        t_values = {1: 12.7, 2: 4.3, 3: 3.2, 4: 2.8, 5: 2.6, 6: 2.4, 7: 2.4, 8: 2.3, 9: 2.3, 10: 2.2}
        t_value = t_values.get(n, 2.0)  # Default para n > 10
        
        margin_error = t_value * (std_dev / (n ** 0.5)) if n > 0 else 0
        ci_lower = max(0, mean_time - margin_error)
        ci_upper = mean_time + margin_error
        
        # Detec√ß√£o de outliers (usando IQR method)
        outliers_count = 0
        if len(times) >= 4:
            sorted_times = sorted(times)
            q1 = sorted_times[len(sorted_times)//4]
            q3 = sorted_times[3*len(sorted_times)//4]
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outliers_count = sum(1 for t in times if t < lower_bound or t > upper_bound)
        
        # Estimativa de mem√≥ria (simulada baseada no tamanho do texto)
        # Em um cen√°rio real, usaria psutil ou tracemalloc
        memory_mb = min(0.1 + (len(times) * 0.01), 2.0)  # Estimativa conservadora
        
        return {
            "ci_lower": ci_lower,
            "ci_upper": ci_upper,
            "outliers_count": outliers_count,
            "memory_mb": memory_mb
        }
    
    def _consolidate_analysis(self) -> Dict[str, Any]:
        """Consolida an√°lise de todos os resultados."""
        if not self.results:
            return {"error": "Nenhum resultado dispon√≠vel para an√°lise"}
        
        # Separar por configura√ß√£o e dataset
        by_config = {}
        by_dataset = {}
        by_size = {}
        by_language = {}
        
        all_times = []
        all_quality_scores = []
        
        for result_key, result in self.results.items():
            if result["status"] != "success":
                continue
                
            config_name = result["config"]["name"]
            dataset_name = result["dataset"]["name"] 
            dataset_size = result["dataset"]["size"]
            dataset_lang = result["dataset"]["language"]
            
            # Agrupar por configura√ß√£o
            if config_name not in by_config:
                by_config[config_name] = []
            by_config[config_name].append(result)
            
            # Agrupar por dataset
            if dataset_name not in by_dataset:
                by_dataset[dataset_name] = []
            by_dataset[dataset_name].append(result)
            
            # Agrupar por tamanho
            if dataset_size not in by_size:
                by_size[dataset_size] = []
            by_size[dataset_size].append(result)
            
            # Agrupar por idioma
            if dataset_lang not in by_language:
                by_language[dataset_lang] = []
            by_language[dataset_lang].append(result)
            
            # Coletar m√©tricas globais
            all_times.append(result["performance"]["avg_time_ms"])
            if "coverage_ratio" in result["quality"].get("coverage", {}):
                all_quality_scores.append(result["quality"]["coverage"]["coverage_ratio"])
        
        # An√°lise consolidada
        consolidated = {
            "summary": {
                "total_tests": len(self.results),
                "successful_tests": len([r for r in self.results.values() if r["status"] == "success"]),
                "failed_tests": len([r for r in self.results.values() if r["status"] != "success"]),
                "configurations_tested": len(by_config),
                "datasets_tested": len(by_dataset)
            },
            "performance": {
                "overall_avg_time_ms": statistics.mean(all_times) if all_times else 0,
                "overall_median_time_ms": statistics.median(all_times) if all_times else 0,
                "fastest_time_ms": min(all_times) if all_times else 0,
                "slowest_time_ms": max(all_times) if all_times else 0,
                "by_config": self._analyze_by_group(by_config),
                "by_dataset_size": self._analyze_by_group(by_size),
                "by_language": self._analyze_by_group(by_language)
            },
            "quality": {
                "overall_avg_coverage": statistics.mean(all_quality_scores) if all_quality_scores else 0,
                "by_config": self._analyze_quality_by_group(by_config),
                "by_dataset_size": self._analyze_quality_by_group(by_size)
            },
            "detailed_results": self.results,
            "execution_metadata": self.execution_metadata,
            "timestamp": datetime.now().isoformat()
        }
        
        return consolidated
    
    def _analyze_by_group(self, grouped_results: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """Analisa performance por grupo."""
        analysis = {}
        
        for group_name, results in grouped_results.items():
            times = [r["performance"]["avg_time_ms"] for r in results if r["status"] == "success"]
            
            if times:
                analysis[group_name] = {
                    "count": len(times),
                    "avg_time_ms": statistics.mean(times),
                    "median_time_ms": statistics.median(times),
                    "min_time_ms": min(times),
                    "max_time_ms": max(times),
                    "std_dev_ms": statistics.stdev(times) if len(times) > 1 else 0
                }
        
        return analysis
    
    def _analyze_quality_by_group(self, grouped_results: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """Analisa qualidade por grupo."""
        analysis = {}
        
        for group_name, results in grouped_results.items():
            coverages = []
            keyword_counts = []
            
            for r in results:
                if r["status"] == "success":
                    if "coverage_ratio" in r["quality"].get("coverage", {}):
                        coverages.append(r["quality"]["coverage"]["coverage_ratio"])
                    keyword_counts.append(r["quality"]["keywords_count"])
            
            if coverages or keyword_counts:
                analysis[group_name] = {
                    "count": len(results),
                    "avg_coverage": statistics.mean(coverages) if coverages else 0,
                    "avg_keywords": statistics.mean(keyword_counts) if keyword_counts else 0,
                    "coverage_samples": len(coverages),
                    "keyword_samples": len(keyword_counts)
                }
        
        return analysis
    
    def _save_results(self, consolidated_analysis: Dict[str, Any]) -> str:
        """Salva resultados em arquivo JSON."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Gerar hash dos resultados para detec√ß√£o de mudan√ßas
        results_str = json.dumps(consolidated_analysis, sort_keys=True, default=str)
        results_hash = hashlib.md5(results_str.encode()).hexdigest()[:8]
        
        filename = f"yake_benchmark_definitivo_{timestamp}_{results_hash}.json"
        output_path = self.output_dir / filename
        
        # Dados a salvar
        output_data = {
            "benchmark_type": "definitivo_robusto",
            "version": "2.0",
            "timestamp": timestamp,
            "results_hash": results_hash,
            "data": consolidated_analysis
        }
        
        # Salvar JSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)
        
        # Criar link para √∫ltimo resultado
        latest_path = self.output_dir / "latest_benchmark_definitivo.json"
        try:
            if latest_path.exists():
                latest_path.unlink()
            # Criar copy ao inv√©s de symlink para compatibilidade Windows
            with open(latest_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)
        except Exception:
            pass  # Falha silenciosa se n√£o conseguir criar link
        
        return str(output_path)
    
    def compare_with_baseline(self, baseline_file: str) -> Dict[str, Any]:
        """
        Compara resultados atuais com baseline.
        
        Args:
            baseline_file: Caminho para arquivo de baseline
            
        Returns:
            An√°lise comparativa
        """
        try:
            with open(baseline_file, 'r', encoding='utf-8') as f:
                baseline_data = json.load(f)
            
            baseline_results = baseline_data.get("data", {})
            current_results = self._consolidate_analysis()
            
            # Compara√ß√£o de performance
            performance_comparison = self._compare_performance(
                baseline_results.get("performance", {}),
                current_results.get("performance", {})
            )
            
            # Compara√ß√£o de qualidade
            quality_comparison = self._compare_quality(
                baseline_results.get("quality", {}),
                current_results.get("quality", {})
            )
            
            return {
                "baseline_file": baseline_file,
                "baseline_timestamp": baseline_data.get("timestamp", "unknown"),
                "current_timestamp": datetime.now().isoformat(),
                "performance": performance_comparison,
                "quality": quality_comparison,
                "summary": self._generate_comparison_summary(performance_comparison, quality_comparison)
            }
            
        except Exception as e:
            return {
                "error": f"Erro ao comparar com baseline: {e}",
                "baseline_file": baseline_file
            }
    
    def _compare_performance(self, baseline: Dict, current: Dict) -> Dict[str, Any]:
        """Compara m√©tricas de performance."""
        comparison = {}
        
        # M√©tricas principais
        for metric in ["overall_avg_time_ms", "overall_median_time_ms"]:
            if metric in baseline and metric in current:
                baseline_val = baseline[metric]
                current_val = current[metric]
                change = current_val - baseline_val
                change_pct = (change / baseline_val * 100) if baseline_val != 0 else 0
                
                comparison[metric] = {
                    "baseline": baseline_val,
                    "current": current_val,
                    "change": change,
                    "change_percent": change_pct,
                    "improvement": change < 0  # Menor tempo √© melhor
                }
        
        return comparison
    
    def _compare_quality(self, baseline: Dict, current: Dict) -> Dict[str, Any]:
        """Compara m√©tricas de qualidade."""
        comparison = {}
        
        # M√©trica principal de cobertura
        if "overall_avg_coverage" in baseline and "overall_avg_coverage" in current:
            baseline_val = baseline["overall_avg_coverage"]
            current_val = current["overall_avg_coverage"]
            change = current_val - baseline_val
            change_pct = (change / baseline_val * 100) if baseline_val != 0 else 0
            
            comparison["overall_avg_coverage"] = {
                "baseline": baseline_val,
                "current": current_val,
                "change": change,
                "change_percent": change_pct,
                "improvement": change > 0  # Maior cobertura √© melhor
            }
        
        return comparison
    
    def _generate_comparison_summary(self, perf_comp: Dict, qual_comp: Dict) -> Dict[str, Any]:
        """Gera resumo da compara√ß√£o."""
        summary = {
            "performance_status": "unknown",
            "quality_status": "unknown",
            "overall_status": "unknown",
            "recommendations": []
        }
        
        # An√°lise de performance
        if "overall_avg_time_ms" in perf_comp:
            change_pct = perf_comp["overall_avg_time_ms"]["change_percent"]
            if change_pct < -5:  # Melhoria > 5%
                summary["performance_status"] = "improved"
            elif change_pct > 5:  # Degrada√ß√£o > 5%
                summary["performance_status"] = "degraded"
                summary["recommendations"].append("Investigar degrada√ß√£o de performance")
            else:
                summary["performance_status"] = "stable"
        
        # An√°lise de qualidade
        if "overall_avg_coverage" in qual_comp:
            change_pct = qual_comp["overall_avg_coverage"]["change_percent"]
            if change_pct > 2:  # Melhoria > 2%
                summary["quality_status"] = "improved"
            elif change_pct < -2:  # Degrada√ß√£o > 2%
                summary["quality_status"] = "degraded"
                summary["recommendations"].append("Investigar degrada√ß√£o de qualidade")
            else:
                summary["quality_status"] = "stable"
        
        # Status geral
        if summary["performance_status"] == "improved" and summary["quality_status"] in ["improved", "stable"]:
            summary["overall_status"] = "improved"
        elif summary["performance_status"] == "degraded" or summary["quality_status"] == "degraded":
            summary["overall_status"] = "degraded"
        else:
            summary["overall_status"] = "stable"
        
        return summary


def main():
    """Fun√ß√£o principal do benchmark definitivo."""
    parser = argparse.ArgumentParser(
        description="YAKE Benchmark Definitivo - Vers√£o Robusta",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python benchmark_definitivo.py                           # Benchmark completo
  python benchmark_definitivo.py --config standard        # Apenas config padr√£o
  python benchmark_definitivo.py --dataset tech           # Apenas datasets de tecnologia
  python benchmark_definitivo.py --output results_custom  # Output customizado
  python benchmark_definitivo.py --compare baseline.json  # Comparar com baseline
  python benchmark_definitivo.py --profiling              # Com profiling detalhado
        """
    )
    
    parser.add_argument("--config", help="Filtro para configura√ß√µes espec√≠ficas")
    parser.add_argument("--dataset", help="Filtro para datasets espec√≠ficos")
    parser.add_argument("--output", default="results", help="Diret√≥rio de output")
    parser.add_argument("--compare", help="Arquivo baseline para compara√ß√£o")
    parser.add_argument("--profiling", action="store_true", help="Habilitar profiling detalhado")
    
    args = parser.parse_args()
    
    try:
        # Criar benchmark
        benchmark = BenchmarkDefinitivo(
            output_dir=args.output,
            enable_profiling=args.profiling
        )
        
        # Executar benchmark
        results = benchmark.run_benchmark(
            config_filter=args.config,
            dataset_filter=args.dataset
        )
        
        # Compara√ß√£o com baseline (se especificado)
        if args.compare:
            print(f"\nüìä Comparando com baseline: {args.compare}")
            comparison = benchmark.compare_with_baseline(args.compare)
            
            if "error" not in comparison:
                print(f"üìà Status geral: {comparison['summary']['overall_status']}")
                if comparison['summary']['recommendations']:
                    print("‚ö†Ô∏è  Recomenda√ß√µes:")
                    for rec in comparison['summary']['recommendations']:
                        print(f"   ‚Ä¢ {rec}")
            else:
                print(f"‚ùå {comparison['error']}")
        
        print(f"\nüéâ Benchmark definitivo conclu√≠do com sucesso!")
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è  Benchmark interrompido pelo usu√°rio")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Erro durante execu√ß√£o: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()